# -*- coding: utf-8 -*-
"""etl_pipeline_script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aT9mkFRUGXhiofATyWN_H3i-defDNyAI
"""

# Install necessary libraries
!pip install pandas numpy matplotlib seaborn

# Import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
pd.set_option('display.max_columns', 200)

df = pd.read_csv('/content/Chicago_Crimes_2012_to_2017.csv')

df.shape

df.head()

# Check missing coordinates
print("Missing Latitude:", df['Latitude'].isnull().sum())
print("Missing Longitude:", df['Longitude'].isnull().sum())

# Drop rows with missing coordinates
df = df.dropna(subset=['Latitude', 'Longitude'])

# Optionally: Remove coordinates that are clearly invalid (optional)
df = df[(df['Latitude'].between(-90, 90)) & (df['Longitude'].between(-180, 180))]

print("Shape after cleaning coordinates:", df.shape)

# Parse Date with day-first
df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')

# Now split into parts
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['Hour'] = df['Date'].dt.hour
df['Weekday'] = df['Date'].dt.day_name()

# Quick check
df[['Date', 'Year', 'Month', 'Day', 'Hour', 'Weekday']].head()

df.head()

df[['Date', 'Year', 'Month', 'Day', 'Hour', 'Weekday']].head()

# Remove exact duplicate rows
df = df.drop_duplicates()
print("Shape after dropping duplicates:", df.shape)

df.describe()

df.info()

# Standardize 'Primary Type'
df['Primary Type'] = df['Primary Type'].str.strip().str.title()

# Standardize 'Location Description'
# First fill missing with 'Unknown' (because 1418405 non-null out of 1419631)
df['Location Description'] = df['Location Description'].fillna('Unknown')
df['Location Description'] = df['Location Description'].str.strip().str.title()

# Quick check
df[['Primary Type', 'Location Description']].head(10)

# Before Cleaning - Sample view
print("Before Cleaning:")
print(df[['Primary Type', 'Location Description']].sample(5))

# Standardize
df['Primary Type'] = df['Primary Type'].str.strip().str.title()
df['Location Description'] = df['Location Description'].fillna('Unknown')
df['Location Description'] = df['Location Description'].str.strip().str.title()

# After Cleaning - Sample view
print("\nAfter Cleaning:")
print(df[['Primary Type', 'Location Description']].sample(5))

# Check for full duplicates
print("Full duplicate rows before:", df.duplicated().sum())

df.shape

# Check for full duplicates
print("Full duplicate rows before:", df.duplicated().sum())

# Remove full duplicate rows
df = df.drop_duplicates()

# Confirm
print("Full duplicate rows after:", df.duplicated().sum())
print("New shape after dropping duplicates:", df.shape)

# Check if 'ID' has duplicates
print("Duplicate IDs:", df['ID'].duplicated().sum())

# Check if 'Case Number' has duplicates
print("Duplicate Case Numbers:", df['Case Number'].duplicated().sum())

# Group by Community Area and count incidents
spatial_density = df.groupby('Community Area').size().reset_index(name='Incidents_Per_Community')

# Merge this density back into main DataFrame
df = df.merge(spatial_density, on='Community Area', how='left')

# Quick check
df[['Community Area', 'Incidents_Per_Community']].head(10)

# Step 1: Make sure 'Date' is the index
df = df.set_index('Date')

# Step 2: Create a daily incidents DataFrame
daily_incidents = df.resample('D').size()

# Step 3: Calculate the 7-day rolling average
rolling_7day_avg = daily_incidents.rolling(window=7, min_periods=1).mean()

# Step 4: Merge the rolling average back into the original DataFrame
# First reset index because our main df needs Date as a column
df = df.reset_index()

# Merge the rolling 7-day average into the main DataFrame
# We map the rolling average value for each date
df['Rolling_7day_Avg'] = df['Date'].map(rolling_7day_avg)

# Quick check
df[['Date', 'Rolling_7day_Avg']].head(10)

# Extract just the date part (no time)
df['Date_only'] = df['Date'].dt.date

# Group incidents by Date_only
daily_incidents = df.groupby('Date_only').size()

# Calculate rolling average
rolling_7day_avg = daily_incidents.rolling(window=7, min_periods=1).mean()

# Map rolling average back to the original dataframe
df['Rolling_7day_Avg'] = df['Date_only'].map(rolling_7day_avg)

# Quick check
df[['Date', 'Date_only', 'Rolling_7day_Avg']].head(10)

# prompt: continue with feature 3:
# Weekend/weekday crime flags

# Create 'Is_Weekend' flag
df['Is_Weekend'] = df['Weekday'].isin(['Saturday', 'Sunday'])

# Create 'Is_Weekday' flag (opposite of 'Is_Weekend')
df['Is_Weekday'] = ~df['Is_Weekend']

# Quick check
df[['Weekday', 'Is_Weekend', 'Is_Weekday']].head(10)

# prompt: continue with feature 4:
# Repeat incident probability

# Calculate the probability of an incident happening on each day of the week.
weekday_probabilities = df.groupby('Weekday')['ID'].count() / len(df)

# Print the probabilities.
print(weekday_probabilities)

# Function to predict the probability of an incident based on the day of the week
def predict_incident_probability(day_of_week):
    """Predicts the probability of an incident based on the day of the week.

    Args:
        day_of_week: The day of the week (e.g., 'Monday', 'Tuesday').

    Returns:
        The probability of an incident on that day of the week, or None if the day is invalid.
    """
    day_of_week = day_of_week.title()  # Standardize input
    if day_of_week in weekday_probabilities:
        return weekday_probabilities[day_of_week]
    else:
        return None

# Example usage:
probability_on_monday = predict_incident_probability('monday')
print(f"Probability of an incident on Monday: {probability_on_monday}")

# prompt: Reshape step by step,
#  data for analysis:
# ■ Unpivot crime counts
# ■ Pivot monthly summaries for time-series analysis

import pandas as pd
# Unpivot crime counts (assuming you want to unpivot 'Primary Type' counts)
# First, group by date and crime type, then count occurrences
crime_counts = df.groupby(['Date', 'Primary Type']).size().reset_index(name='Counts')

# Now, pivot the table to show monthly crime summaries
monthly_crimes = crime_counts.groupby(['Date', 'Primary Type'])['Counts'].sum().reset_index()

# Convert 'Date' to datetime if it isn't already
monthly_crimes['Date'] = pd.to_datetime(monthly_crimes['Date'])


# Extract year and month
monthly_crimes['Year'] = monthly_crimes['Date'].dt.year
monthly_crimes['Month'] = monthly_crimes['Date'].dt.month

# Group by year, month, and primary type and sum the counts
monthly_summaries = monthly_crimes.groupby(['Year', 'Month', 'Primary Type'])['Counts'].sum().reset_index()

# Pivot the monthly summaries for time-series analysis
monthly_pivot = monthly_summaries.pivot_table(index=['Year', 'Month'], columns='Primary Type', values='Counts', fill_value=0)

# Display the pivoted data
print(monthly_pivot.head())

print(monthly_pivot.index.get_level_values('Year').unique())

monthly_2013 = monthly_pivot.loc[2013]
print(monthly_2013)

monthly_2012_2013 = monthly_pivot.loc[[2012, 2013]]
print(monthly_2012_2013)

jan_2012 = monthly_pivot.loc[(2012, 12)]
print(jan_2012)

monthly_pivot_reset = monthly_pivot.reset_index()
print(monthly_pivot_reset.head())

import sqlite3

!pip install psycopg2-binary

import sqlite3

# Connect to a local SQLite database
db_path = 'crime_data.db'  # You can make this configurable
conn = sqlite3.connect(db_path)

# Get unique crime types
crime_types = df[['Primary Type']].drop_duplicates().reset_index(drop=True)

# Add a Primary_Type_ID
crime_types['Primary_Type_ID'] = crime_types.index + 1

# Reorder columns
crime_types = crime_types[['Primary_Type_ID', 'Primary Type']]

crime_types.head()

# Get unique location descriptions
locations = df[['Location Description']].drop_duplicates().reset_index(drop=True)

# Add a Location_ID
locations['Location_ID'] = locations.index + 1

# Reorder columns
locations = locations[['Location_ID', 'Location Description']]

locations.head()

# Merge with Crime_Types to get Primary_Type_ID
df = df.merge(crime_types, on='Primary Type', how='left')

# Merge with Locations to get Location_ID
df = df.merge(locations, on='Location Description', how='left')

# Create Incident_ID
df['Incident_ID'] = df.index + 1

# Select only relevant columns
incidents = df[['Incident_ID', 'Date', 'Primary_Type_ID', 'Location_ID']]

incidents.head()

# Save tables
crime_types.to_sql('Crime_Types', conn, index=False, if_exists='replace')
locations.to_sql('Locations', conn, index=False, if_exists='replace')
incidents.to_sql('Incidents', conn, index=False, if_exists='replace')

# Create a cursor
cursor = conn.cursor()

# Example: View first 5 incidents
cursor.execute('SELECT * FROM Incidents LIMIT 5;')
for row in cursor.fetchall():
    print(row)

query = """
SELECT i.Incident_ID, i.Date, ct."Primary Type", l."Location Description"
FROM Incidents i
JOIN Crime_Types ct ON i.Primary_Type_ID = ct.Primary_Type_ID
JOIN Locations l ON i.Location_ID = l.Location_ID
LIMIT 10;
"""

for row in cursor.execute(query):
    print(row)

